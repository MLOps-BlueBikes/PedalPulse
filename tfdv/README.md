Bluebikes Data Validation and Processing Pipeline
=================================================

This project is designed to validate, process, and analyze monthly Bluebikes trip data stored in a Google Cloud Platform (GCP) bucket. Using the Pandera and TensorFlow Data Validation (TFDV) libraries, this pipeline validates data schema, generates statistics, and infers schemas from the dataset. The pipeline processes data for each month within a specified date range and outputs validation statistics and inferred schemas as .pbtxt files.

Table of Contents
-----------------

1.  [Project Overview](#project-overview)
    
2.  [Requirements](#requirements)
    
3.  [Setup](#setup)
    
4.  [Script Details](#script-details)
    
5.  [Usage](#usage)
    
6.  [Output](#output)
    
7.  [License](#license)
    

Project Overview
----------------

The Bluebikes Data Validation and Processing Pipeline accomplishes the following tasks:

1.  **Data Loading**: Fetches monthly CSV data files from a specified GCP bucket.
    
2.  **Data Validation**: Uses Pandera to validate data schema, ensuring required columns, data types, and value constraints are met.
    
3.  **Statistics Generation**: Utilizes TensorFlow Data Validation to generate statistics for each month.
    
4.  **Schema Inference**: Infers data schema based on generated statistics and saves schema files for each month.
    

This pipeline can be used to monitor the quality of data stored in GCP and help identify potential schema drifts or anomalies in the dataset over time.

Requirements
------------

*   **Python 3.7+**
    
*   **Google Cloud SDK** (with storage client library)
    
*   **Python Packages**:
    
    *   pandas
        
    *   google-cloud-storage
        
    *   tensorflow\_data\_validation
        
    *   pandera
        

To install the required Python packages, run:

pip install pandas google-cloud-storage tensorflow_data_validation pandera

## Setup

1. **Google Cloud Authentication**: Set up your Google Cloud credentials by saving the JSON key file to a secure location and then set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable:

   ```bash
   export GOOGLE_APPLICATION_CREDENTIALS="./data/pedal-pulse-raw-data-5b8626b891ce.json"

Script Details
--------------

*   **read\_from\_gcp\_bucket**: Fetches a CSV file from the GCP bucket.
    
*   **validate\_schema**: Validates the dataframe using a predefined Pandera schema. This schema includes column constraints, data types, and specific value checks (e.g., "member\_casual" must be either "member" or "casual").
    
*   **process\_monthly\_data**:
    
    *   Loads each month’s data.
        
    *   Validates it against the schema.
        
    *   Generates and saves statistics and schema using TensorFlow Data Validation.
        
*   **main**: Executes the pipeline for a specified date range and bucket name.
    

Usage
-----

To run the script, use:
# Data Fetching Script

This script fetches data for each month within a specified date range, validates the data, and generates statistics and schema files.

## Usage

To run the script, use the following command in your terminal:

python your_script_name.py

The script fetches data for each month in the specified date range (from start\_month to end\_month), validates it, and generates statistics and schema files.

### Configuration

Update the following parameters as needed:

*   **GCP Bucket Name**: Modify bucket\_name in the main function with your GCP bucket name.
    
*   **Date Range**: Adjust start\_month and end\_month in main to specify the desired date range.
    

Output
------

The script outputs the following files:

*   **Monthly Statistics**: statistics\_.pbtxt — Statistics file for each month, generated by TFDV.
    
*   **Monthly Schema**: schema\_.pbtxt — Schema inferred by TFDV based on the monthly data.
    

Both files are saved in the working directory.
